{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Required libraries**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "e9wH5bYVefhA"
   },
   "source": [
    "# Just run this block. Please do not modify the following code.\n",
    "import math\n",
    "import time\n",
    "import io\n",
    "import numpy as np\n",
    "import csv\n",
    "from IPython.display import Image\n",
    "\n",
    "# Pytorch package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Torchtest package\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tqdm progress bar\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "# Code provide to you for training and evaluation\n",
    "from utils import train, evaluate, set_seed_nb, unit_test_values, deterministic_init, plot_curves\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "tHOKA0U5efhF"
   },
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "zJXBPulgefhH"
   },
   "source": [
    "# load checkers\n",
    "d1 = torch.load('./data/d1.pt')\n",
    "d2 = torch.load('./data/d2.pt')\n",
    "d3 = torch.load('./data/d3.pt')\n",
    "d4 = torch.load('./data/d4.pt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDmu06h_efhI"
   },
   "source": [
    "## **Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "uWW68l46efhJ"
   },
   "source": [
    "MAX_LEN = 20\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "j5Rx2dPtefhK"
   },
   "source": [
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_.lower()))\n",
    "  return vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=2)\n",
    "\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "de_vocab.set_default_index(de_vocab['<unk>'])\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "fCIRekFBefhL"
   },
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    raw_en_l=raw_en.lower()     #turn sentences to lower case\n",
    "    raw_de_l=raw_de.lower()\n",
    "    de_tensor = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de_l)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en_l)],\n",
    "                            dtype=torch.long)\n",
    "    if len(de_tensor) <= MAX_LEN-2 and len(en_tensor) <= MAX_LEN-2:\n",
    "        data.append((de_tensor, en_tensor))\n",
    "  return data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "SH0hV4WzefhM"
   },
   "source": [
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "KCMNnBQDefhM"
   },
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "SOS_IDX = de_vocab['<sos>']\n",
    "EOS_IDX = de_vocab['<eos>']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ew_LjPRuefhO"
   },
   "source": [
    "def generate_batch(data_batch):\n",
    "\n",
    "    de_batch, en_batch = [], []\n",
    "    for (de_item, en_item) in data_batch:\n",
    "          en_batch.append(torch.cat([torch.tensor([SOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "          de_batch.append(torch.cat([torch.tensor([SOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    fix=torch.ones(MAX_LEN,en_batch.shape[1])\n",
    "    two= pad_sequence([de_batch,en_batch, fix], padding_value=PAD_IDX)\n",
    "    de_batch=two[:,0,]\n",
    "    en_batch=two[:,1,]\n",
    "    return de_batch, en_batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "EyYhQwPvefhO"
   },
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=generate_batch)\n",
    "valid_loader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=generate_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, collate_fn=generate_batch)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "xSE8cUDJefhP"
   },
   "source": [
    "# Get the input and the output sizes for model\n",
    "input_size = len(de_vocab)\n",
    "output_size = len(en_vocab)\n",
    "print (input_size,output_size)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "vQyPYCX6ShY_"
   },
   "source": [
    "def train_and_plot (model, optimizer, scheduler, criterion, filename):\n",
    "  train_perplexity_history = []\n",
    "  valid_perplexity_history = []\n",
    "\n",
    "  for epoch_idx in range(EPOCHS):\n",
    "      print(\"-----------------------------------\")\n",
    "      print(\"Epoch %d\" % (epoch_idx+1))\n",
    "      print(\"-----------------------------------\")\n",
    "\n",
    "      train_loss, avg_train_loss = train(model, train_loader, optimizer, criterion, device=device)\n",
    "      scheduler.step(train_loss)\n",
    "\n",
    "      val_loss, avg_val_loss = evaluate(model, valid_loader, criterion, device=device)\n",
    "\n",
    "      train_perplexity_history.append(np.exp(avg_train_loss))\n",
    "      valid_perplexity_history.append(np.exp(avg_val_loss))\n",
    "\n",
    "      print(\"Training Loss: %.4f. Validation Loss: %.4f. \" % (avg_train_loss, avg_val_loss))\n",
    "      print(\"Training Perplexity: %.4f. Validation Perplexity: %.4f. \" % (np.exp(avg_train_loss), np.exp(avg_val_loss)))\n",
    "\n",
    "  plot_curves(train_perplexity_history, valid_perplexity_history, filename)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VzSCxdPefhd"
   },
   "source": [
    "## **Train a Transformer**\n",
    "\n",
    "We will be implementing a one-layer Transformer **encoder** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "Oh9-f1YRefhd"
   },
   "source": [
    "Image(filename=\"imgs/encoder.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFxaE855efhe"
   },
   "source": [
    "You can refer to the [original paper](https://arxiv.org/pdf/1706.03762.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-S4wWKRefhe"
   },
   "source": [
    "## Load the preprocessed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "1eZXnK7eefhe"
   },
   "source": [
    "train_inxs = np.load('./data/train_inxs.npy')\n",
    "val_inxs = np.load('./data/val_inxs.npy')\n",
    "train_labels = np.load('./data/train_labels.npy')\n",
    "val_labels = np.load('./data/val_labels.npy')\n",
    "\n",
    "# load dictionary\n",
    "word_to_ix = {}\n",
    "with open(\"./data/word_to_ix.csv\", \"r\", encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        word_to_ix[line[0]] = line[1]\n",
    "print(\"Vocabulary Size:\", len(word_to_ix))\n",
    "\n",
    "print(train_inxs.shape) # 7000 training instances, of (maximum/padded) length 43 words.\n",
    "print(val_inxs.shape) # 1551 validation instances, of (maximum/padded) length 43 words.\n",
    "print(train_labels.shape)\n",
    "print(val_labels.shape)\n",
    "\n",
    "d1 = torch.load('./data/d1.pt')\n",
    "d2 = torch.load('./data/d2.pt')\n",
    "d3 = torch.load('./data/d3.pt')\n",
    "d4 = torch.load('./data/d4.pt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUUUtdmbefhf"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N49WZFwdefhf"
   },
   "source": [
    "## **Embeddings**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "jBE-urJ6efhf"
   },
   "source": [
    "Image(filename=\"imgs/embedding.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsNweNb6efhf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "HoelwPQSefhg"
   },
   "source": [
    "from models.Transformer import TransformerTranslator\n",
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)\n",
    "\n",
    "model = TransformerTranslator(input_size=len(word_to_ix), output_size=2, device=device, hidden_dim=128, num_heads=2, dim_feedforward=2048, dim_k=96, dim_v=96, dim_q=96, max_length=train_inxs.shape[1])\n",
    "\n",
    "embeds = model.embed(inputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(embeds, d1)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPrD-eJoefhg"
   },
   "source": [
    "## **Multi-head Self-Attention**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "2R955b8Hefhg"
   },
   "source": [
    "Image(filename=\"imgs/attn.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN2AolSLefhg"
   },
   "source": [
    "We want to have multiple self-attention operations, computed in parallel. Each of these is called a *head*. We concatenate the heads and multiply them with the matrix `attention_head_projection` to produce the output of this layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "eWCXx14cefhh"
   },
   "source": [
    "Image(filename=\"imgs/layer_norm.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIuT253Iefhh"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "QX4cYmZ3efhh"
   },
   "source": [
    "hidden_states = model.multi_head_attention(embeds)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(hidden_states, d2)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS0afPPDefhh"
   },
   "source": [
    "## **Element-Wise Feed-forward Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "7__n-8puefhi"
   },
   "source": [
    "Image(filename=\"imgs/ffn.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "f4hxfoMaefhi"
   },
   "source": [
    "outputs = model.feedforward_layer(hidden_states)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(outputs, d3)).item()) # should be very small (<0.01)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oU3skwOefhi"
   },
   "source": [
    "## **Final Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "source": [
    "print(outputs.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "D3De-L1Yefhi"
   },
   "source": [
    "\n",
    "scores = model.final_layer(outputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(scores, d4)).item()) # should be very small (<3e-4)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "h3OiFe6Defhj"
   },
   "source": [
    "inputs = train_inxs[0:2]\n",
    "inputs = torch.LongTensor(inputs)\n",
    "inputs.to(device)\n",
    "outputs = model.forward(inputs)\n",
    "\n",
    "try:\n",
    "    print(\"Difference:\", torch.sum(torch.pairwise_distance(outputs, scores)).item()) # should be very small (<3e-4)\n",
    "except:\n",
    "    print(\"NOT IMPLEMENTED\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr_Ehyqvefhk"
   },
   "source": [
    "## **Train the Transformer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "cHpZSsPQefhk"
   },
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "EPOCHS = 10\n",
    "\n",
    "# Model\n",
    "trans_model = TransformerTranslator(input_size, output_size, device, max_length = MAX_LEN).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.Adam(trans_model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "biCi-lFHefhk"
   },
   "source": [
    "filename='trans_model'\n",
    "train_and_plot(trans_model, optimizer, scheduler, criterion, filename)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmdFF3d7vqaQ"
   },
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dfJtyp5Kefgy",
    "tmSlhfxwefg9",
    "O66UKOZnefg_",
    "FDmu06h_efhI",
    "l6yEbmAxefhR",
    "cTkd6oseefhS",
    "zZwBWMZzefhU",
    "v7zKygJLefhW",
    "oUL7qUZKefhW",
    "o7XOnp0LefhX",
    "fBXsJFeTefhZ",
    "XYB7O3D1efha",
    "_VzSCxdPefhd",
    "qVs0KDP0efhe",
    "z-S4wWKRefhe",
    "N49WZFwdefhf",
    "CPrD-eJoefhg",
    "aS0afPPDefhh",
    "-oU3skwOefhi",
    "KqV-7w2wefhj",
    "pr_Ehyqvefhk"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "interpreter": {
   "hash": "0e75f62e3678e2cc45ba815b06d45149f3ef8e725365fb50a06024c1d0abc38d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
